{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a294105",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 301 - Crying baby: 100%|██████████| 108/108 [00:27<00:00,  3.89it/s]\n",
      "Processing 901 - Silence: 100%|██████████| 108/108 [00:02<00:00, 46.59it/s]\n",
      "Processing 902 - Noise: 100%|██████████| 108/108 [00:19<00:00,  5.49it/s]\n",
      "Processing 903 - Baby laugh: 100%|██████████| 108/108 [00:02<00:00, 46.90it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pydub import AudioSegment\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from tqdm import tqdm\n",
    "\n",
    "SOURCE_DIRS = {\n",
    "    \"301 - Crying baby\": 1,\n",
    "    \"901 - Silence\": 0,\n",
    "    \"902 - Noise\": 0,\n",
    "    \"903 - Baby laugh\": 0,\n",
    "}\n",
    "OUTPUT_DIR = \"data\"\n",
    "SAMPLE_RATE = 16000\n",
    "CHUNK_DURATION = 3  # seconds\n",
    "\n",
    "os.makedirs(f\"{OUTPUT_DIR}/crying\", exist_ok=True)\n",
    "os.makedirs(f\"{OUTPUT_DIR}/not_crying\", exist_ok=True)\n",
    "\n",
    "def convert_to_wav(src_path):\n",
    "    if src_path.endswith(\".wav\"):\n",
    "        return src_path\n",
    "    audio = AudioSegment.from_file(src_path)\n",
    "    wav_path = src_path.rsplit(\".\", 1)[0] + \".wav\"\n",
    "    audio.export(wav_path, format=\"wav\")\n",
    "    return wav_path\n",
    "\n",
    "def process_file(file_path, label, output_count):\n",
    "    wav_path = convert_to_wav(file_path)\n",
    "    y, sr = librosa.load(wav_path, sr=SAMPLE_RATE, mono=True)\n",
    "    total_duration = librosa.get_duration(y=y, sr=sr)\n",
    "    num_chunks = int(total_duration // CHUNK_DURATION)\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        start_sample = int(i * CHUNK_DURATION * sr)\n",
    "        end_sample = int((i + 1) * CHUNK_DURATION * sr)\n",
    "        chunk = y[start_sample:end_sample]\n",
    "\n",
    "        class_folder = \"crying\" if label == 1 else \"not_crying\"\n",
    "        chunk_path = os.path.join(OUTPUT_DIR, class_folder, f\"{class_folder}_{output_count}.wav\")\n",
    "        sf.write(chunk_path, chunk, sr)\n",
    "        output_count += 1\n",
    "\n",
    "    return output_count\n",
    "\n",
    "def preprocess_all():\n",
    "    output_count = 0\n",
    "    for folder, label in SOURCE_DIRS.items():\n",
    "        folder_path = os.path.join(\".\", folder)\n",
    "        files = [f for f in os.listdir(folder_path) if f.endswith((\".ogg\", \".wav\"))]\n",
    "        for f in tqdm(files, desc=f\"Processing {folder}\"):\n",
    "            full_path = os.path.join(folder_path, f)\n",
    "            output_count = process_file(full_path, label, output_count)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocess_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05d4f02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting from data\\crying: 100%|██████████| 108/108 [00:01<00:00, 68.51it/s]\n",
      "Extracting from data\\not_crying: 100%|██████████| 324/324 [00:04<00:00, 69.80it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "INPUT_DIR = \"data\"\n",
    "OUTPUT_DIR = \"features\"\n",
    "SAMPLE_RATE = 16000\n",
    "N_MELS = 128\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "metadata = []\n",
    "\n",
    "def extract_features(file_path):\n",
    "    y, sr = librosa.load(file_path, sr=SAMPLE_RATE, mono=True)\n",
    "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=N_MELS)\n",
    "    log_mel = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    delta = librosa.feature.delta(log_mel)\n",
    "    delta2 = librosa.feature.delta(log_mel, order=2)\n",
    "    feature_stack = np.stack([log_mel, delta, delta2], axis=0)  # shape: (3, 128, T)\n",
    "    return feature_stack\n",
    "\n",
    "\n",
    "def process_folder(folder, label):\n",
    "    files = [f for f in os.listdir(folder) if f.endswith(\".wav\")]\n",
    "    for f in tqdm(files, desc=f\"Extracting from {folder}\"):\n",
    "        file_path = os.path.join(folder, f)\n",
    "        features = extract_features(file_path)\n",
    "        out_name = f.replace(\".wav\", \".npy\")\n",
    "        out_path = os.path.join(OUTPUT_DIR, out_name)\n",
    "        np.save(out_path, features)\n",
    "        metadata.append({\"path\": out_path, \"label\": label})\n",
    "\n",
    "def main():\n",
    "    process_folder(os.path.join(INPUT_DIR, \"crying\"), 1)\n",
    "    process_folder(os.path.join(INPUT_DIR, \"not_crying\"), 0)\n",
    "    pd.DataFrame(metadata).to_csv(os.path.join(OUTPUT_DIR, \"metadata.csv\"), index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01707d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CryDataset(Dataset):\n",
    "    def __init__(self, csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        self.paths = df['path'].tolist()\n",
    "        self.labels = df['label'].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = np.load(self.paths[idx])\n",
    "        features = torch.tensor(features, dtype=torch.float32)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return features, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83c2f95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.bce = nn.BCELoss(reduction='none')\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = self.bce(inputs, targets)\n",
    "        pt = torch.where(targets == 1, inputs, 1 - inputs)\n",
    "        loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n",
    "        return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9cdd7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "        )\n",
    "        self.lstm = nn.LSTM(input_size=32 * 32, hidden_size=64, num_layers=1,\n",
    "                            batch_first=True, bidirectional=True)\n",
    "        self.attn = nn.Linear(128, 1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 3, 128, T)\n",
    "        x = self.cnn(x)                      # → (B, 32, H, W)\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.permute(0, 3, 1, 2).reshape(B, W, C * H)  # (B, T, Features)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        attn_weights = torch.softmax(self.attn(lstm_out), dim=1)  # (B, T, 1)\n",
    "        context = torch.sum(attn_weights * lstm_out, dim=1)       # (B, 128)\n",
    "        return self.fc(context).squeeze(1)  # (B,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07be4b79",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m      6\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# makes local files importable\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CryDataset\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CryClassifier\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfocal_loss\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FocalLoss\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dataset'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import sys\n",
    "sys.path.append(\".\")  # makes local files importable\n",
    "\n",
    "from dataset import CryDataset\n",
    "from model import CryClassifier\n",
    "from focal_loss import FocalLoss\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 25\n",
    "LR = 1e-4\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset = CryDataset(\"features/metadata.csv\")\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "model = CryClassifier().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = FocalLoss()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X, y in train_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        preds = model(X)\n",
    "        loss = criterion(preds, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    preds, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for X, y in val_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            p = model(X)\n",
    "            preds += (p > 0.5).int().cpu().numpy().tolist()\n",
    "            targets += y.int().cpu().numpy().tolist()\n",
    "\n",
    "    acc = accuracy_score(targets, preds)\n",
    "    f1 = f1_score(targets, preds)\n",
    "    print(f\"[{epoch+1}/{EPOCHS}] Loss: {total_loss:.4f} | Acc: {acc:.3f} | F1: {f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92badf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\".\")  # makes local files importable\n",
    "\n",
    "from dataset import CryDataset\n",
    "from model import CryClassifier\n",
    "from focal_loss import FocalLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68735f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ramil\\AppData\\Local\\Temp\\ipykernel_31664\\1959861477.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"cry_model.pth\", map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CryClassifier(\n",
       "  (cnn): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (lstm): LSTM(1024, 64, batch_first=True, bidirectional=True)\n",
       "  (attn): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=64, out_features=1, bias=True)\n",
       "    (4): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from model import CryClassifier  # Make sure this file is available\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CryClassifier().to(device)\n",
    "model.load_state_dict(torch.load(\"cry_model.pth\", map_location=device))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77c096c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "from dataset import CryDataset\n",
    "\n",
    "dataset = CryDataset(\"features/metadata.csv\")\n",
    "\n",
    "# Same split as before\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_ds, val_test_ds = random_split(dataset, [train_size, val_size + test_size])\n",
    "val_ds, test_ds = random_split(val_test_ds, [val_size, test_size])\n",
    "\n",
    "test_loader = DataLoader(test_ds, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6bf05fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9773\n",
      "F1 Score:  0.9565\n",
      "Precision: 1.0000\n",
      "Recall:    0.9167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjMAAAHHCAYAAABKudlQAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAARNBJREFUeJzt3Qd8U9X7+PHnFqHMlr2k7CUy/ILKUoYyRGUILsAfyNAvypIhyl9BNgoICjJcgANERUHBLyCCgDKUqaiAgGVPQSgUCwXyfz3Hb/JtSluSJmmT3M/b17XNzc29J2lonj7nOedYDofDIQAAACEqIrMbAAAA4AuCGQAAENIIZgAAQEgjmAEAACGNYAYAAIQ0ghkAABDSCGYAAEBII5gBAAAhjWAGAACENIIZIIPt3r1bmjVrJtHR0WJZlixcuNCv59+3b5857+zZs/163lDWqFEjswEITwQzsKW9e/fKv//9bylbtqxkz55doqKipH79+vL666/L33//HdBrd+7cWbZv3y6jR4+WDz74QG699VYJF48//rgJpPT1TOl11EBO79dtwoQJXp//yJEjMmzYMNm2bZsEO22n87mmtfkryPrPf/5jrumpq1evyvvvvy+1a9eW/PnzS548eaRixYrSqVMn2bBhg9fXv3Dhgrn+qlWrvH4s4KsbfD4DEGK++uoreeihhyQyMtL84q5atapcunRJvv/+e3n22Wfl119/lbfeeisg19YP+PXr18sLL7wgvXr1Csg1SpUqZa6TNWtWyQw33HCD+WBbtGiRPPzww273zZkzxwSPCQkJ6Tq3BjPDhw+X0qVLyy233OLx477++mvJaG3btpXy5cu7bp8/f16eeuopeeCBB8x9TkWKFPFbMDN16lSPA5o+ffqY41u3bi0dO3Y0P7ddu3bJkiVLTJBfp04dr66vP3P92SiyYMhoBDOwldjYWHn00UfNB/7KlSulWLFirvt69uwpe/bsMcFOoJw8edJ8zZs3b8CuoX/ta8CQWTRI1CzXRx99dE0wM3fuXLnvvvvks88+y5C26Adszpw5JVu2bJLRqlevbjanP//80wQzuu+xxx6TzHT8+HGZNm2aPPHEE9cE7q+99prrfQqECrqZYCvjxo0zfyG/++67boGMk/4l3bdvX9fty5cvy8iRI6VcuXLmQ1ozAv/v//0/uXjxotvjdP/9999vsju33367CSb0r1tN4zvpX8waRCnNAGnQoY9zds84v0+pqyKp5cuXyx133GECoty5c0ulSpVMm65XM6PB25133im5cuUyj9W/yHfs2JHi9TSo0zbpcVrb06VLFxMYeKpDhw7mL/wzZ8649m3cuNF0M+l9yZ0+fVoGDhwo1apVM89Ju6latGghP/30k+sY7b647bbbzPfaHmc3jfN5ajZAs2ybN2+WBg0amCDG+bokr5nRrj79GSV//s2bN5d8+fKZDFBG2blzpzz44IOmq0fbpN2OX375pdsxiYmJJutRoUIFc0yBAgXMe0DfC0p/VpplUUm7sNIK6h0Ohwk6k9PHFS5c2G2f/hyfeeYZiYmJMf8O9N/JK6+8YrqqnO+5QoUKme+1nc7re9PtBfiCzAxsRbs+NMioV6+eR8d3795d3nvvPfNhM2DAAPnhhx9k7Nix5kNwwYIFbsdqAKDHdevWzXxYzpw503zI1KpVS26++WbTtaDBQb9+/aR9+/Zy7733mg9ub2gXmAZN+tf9iBEjzAeLXnft2rVpPu6bb74xwYE+d/2A0W6oKVOmmA+zLVu2XBNIaUalTJky5rnq/e+88475gNMPME/oc+3Ro4d8/vnn0rVrV1dWpnLlylKzZs1rjv/jjz9MIbR2/+l1NXPw5ptvSsOGDeW3336T4sWLy0033WSe89ChQ+XJJ580gZlK+rM8deqUeZ6afdPsR2pdOFobpcGd/py02y9LlizmetodpXVMer2MoD9P/RnceOON8vzzz5tA85NPPpE2bdqY7JV2SSn9menPQt+PGizHxcXJpk2bzM+madOmpv5LAzANbrT91+MMqj/99FPzmmvglxoNYvXncPjwYXOdkiVLyrp162Tw4MFy9OhRk8nRQGb69OnXdKMlzUwBAeUAbOLs2bMOfcu3bt3ao+O3bdtmju/evbvb/oEDB5r9K1eudO0rVaqU2bdmzRrXvhMnTjgiIyMdAwYMcO2LjY01x40fP97tnJ07dzbnSO6ll14yxztNmjTJ3D558mSq7XZeY9asWa59t9xyi6Nw4cKOU6dOufb99NNPjoiICEenTp2uuV7Xrl3dzvnAAw84ChQokOo1kz6PXLlyme8ffPBBx913322+v3LliqNo0aKO4cOHp/gaJCQkmGOSPw99/UaMGOHat3Hjxmuem1PDhg3NfTNmzEjxPt2SWrZsmTl+1KhRjj/++MORO3duR5s2bRyBoj8zvZ6+xk76+lSrVs08f6erV6866tWr56hQoYJrX40aNRz33Xdfmufv2bOn23vlevTnrsfny5fP/HwnTJjg2LFjxzXHjRw50vxMf//9d7f9zz//vCNLliyOAwcOpPr8gIxCNxNsQ/+aVTpqw9OCStW/f3+3/ZqhUclra6pUqeLKFij9a1W7gDTr4C/OWpsvvvjCleK/Hv3rWUf/aJZIuzKc9K9m/ave+TyT0qxKUvq8NOvhfA09od1J2jV07NgxkwXRryl1MSnNMEVE/PPr6MqVK+Zazi40zT54Ss+jXVCe0OHxmmnQbI9mErT7RrMzGUW71vR10SzYuXPnTE2NbvrctbtLu+Q0G+L8uWsWR/f5y6xZs+SNN94wmTDNMmo3n2a/7r77btd1ndkb/flr95uzjbo1adLE/KzWrFnjtzYB6UUwA9vQOgylHxye2L9/v/mATToiRRUtWtR8uOj9SWn6PTn9APjrr7/EXx555BHTLaHdDdqFot0p2i2RVmDjbKcGBsnph5d+MMXHx6f5XPR5KG+ei3ajaeD48ccfm1FMWu+S/LV00vZPmjTJ1IRoQFKwYEETDP78889y9uxZj6+p3TXeFPvq8HAN8DTYmzx58jW1IinR4lgNzJyb1mClh3YPat3KkCFDzHNNur300kvmmBMnTpivGnBp3YoOnda6Iq250tfGF/re1qJ3rTHS94AGyNpFpwGWvq+cNIBaunTpNW3UYCZpG4HMRM0MbBXMaC3EL7/84tXj0iqkTErrLlKiH1jpvYb+5ZtUjhw5zF/C3377rckM6YeMBgt33XWXqfdIrQ3e8uW5OGlQohkPrTnS7FRaxaBjxowxH+paX6MF1xpg6IetFp16moFyvj7e2Lp1q+vDWOf+0Vqm69GgLGkgq4FHegpdnc9LMyKaiUmJM/jTgmadG0kDDv05aw2TBn8zZswwga2vtKC4VatWZtNC6dWrV5vnqLU12k7N4A0aNCjFx2qABWQ2ghnYihbP6lBULfqsW7dumsc6f5HrX6aawXDS4lT9K9lZROkPmvlIOvLHKXn2R+mHvHYF6DZx4kQTCOi8NRrgOP9aTv48lM4hktJIGs2CaOFpIGi3khZCa5uT/rWf3Pz586Vx48ZmlFlS+ppo+7wNLD2h2SjtktLuQS0i1pFuWrzqHDGVGs0yJZ0QUIuq08P5OJ0PKKWfW3Ia4Gl7ddNskAY4GkQ5gxl/vTY6mkqDGe2e1PeOjuTT612vjf782QDeopsJtqJ/XeoHt34AaFCSnP71qyNdnN0kSkdrJKUBhNL5UvxFPzC0OyVp14F+mCQfMaV1Fsk5J49LPlzcSYeg6zGaIUkaMGmGSv/Kdz7PQNAARTMtWpuh3XNpZYKSZ320ViNp7YZyBl0pBX7eeu655+TAgQPmddGfqY7o0tFNqb2OTtrNpx/szi29wYx2aWkWROt09GedXNK5XrSOJimtJ9KsTdK2evPaaPeYjhJLTiePXLFihVv3qtb0aPC/bNmya47Xa+n0Bco5IsofPxvAW2RmYCsaNOgQYa090WxL0hmAdbipfoBqoayqUaOG+XDTTI7+gtbhqT/++KP58NOhs/pB7S+atdAPV80M6MysOhxWh7pqCj9pAazWTmg3kwZS+lezdpHo5GclSpQw846kZvz48aYeQrNROnTcOTRb55AJ5Fwg+qH44osvepQx0+emWQfNkmiXj2ZAkgcK+vPTeiXtXtF6HP0A1+n4tYjVG1oXoq+bdhE5h4prQawGF9rdpVmajKBzw+jPTetgdAI7fb4aZGvwcOjQIdc8O5o90rbpMH/N0OiwbM1mJZ1FWu9T+v7RbisNEFPLhum5dYi3dk9qhk8DTX0v6USHek3t3nNmxLQ+R+e90Z+Rc6oBzWrpz0jboHPM6LHaxaft1G5Pfd9qO/Xflm5AwGXYuCkgiOgw0yeeeMJRunRpR7Zs2Rx58uRx1K9f3zFlyhS3YbKJiYlmOHGZMmUcWbNmdcTExDgGDx7sdozSYdUpDZ1NPiQ4taHZ6uuvv3ZUrVrVtKdSpUqODz/88Jqh2StWrDBDy4sXL26O06/t27d3Gzab0tBs9c0335jnmCNHDkdUVJSjZcuWjt9++83tGOf1kg/91nPpfj23p0OzU5Pa0Gwdwl6sWDHTPm3n+vXrUxxS/cUXXziqVKniuOGGG9yepx538803p3jNpOeJi4szP6+aNWuan29S/fr1M8PV9dr+ltrQ5b1795ph0jp0Xd9jN954o+P+++93zJ8/33WMDh+//fbbHXnz5jWvT+XKlR2jR492XLp0yXXM5cuXHb1793YUKlTIYVlWmsO09TV4/fXXHc2bN3eUKFHCXFf/DdStW9fx9ttvm+HhSZ07d86878uXL2/edwULFjTDx3U4d9I2rFu3zlGrVi1zDMO0kZEs/V/gQyYAAIDAoGYGAACENIIZAAAQ0ghmAABASCOYAQAAIY1gBgAAhDSCGQAAENKYNC/E6XT7R44cMROIMZ04AIQenSFFF8DVteOcq8cHQkJCgpkg1Fe6mKuuMh9MCGZCnAYyMTExmd0MAICPDh48aGbzDlQgkyNPAZHLF3w+l84YHRsbG1QBDcFMiNOMjMpWpbNYWbJldnOAgDiwakJmNwEImHNxcVK+TIzr93kgXNKMzOULElmls4gvnxVXLsmx394z5yOYgd84u5Y0kCGYQbiKiorK7CYAAZchpQI3ZPfps8JhBWepLcEMAAB2YZmoybfHByGCGQAA7MKK+Gfz5fFBKDhbBQAA4CEyMwAA2IVl+djNFJz9TAQzAADYhUU3EwAAQNAhMwMAgF1YdDMBAICQFuFjV1FwdugEZ6sAAAA8RGYGAAC7sOhmAgAAocxiNBMAAEDQITMDAIBdWHQzAQCAUGaFZzcTwQwAAHZhhWdmJjhDLAAAAA8RzAAAYLduJsuHzQvTp0+X6tWrS1RUlNnq1q0rS5Yscd2fkJAgPXv2lAIFCkju3LmlXbt2cvz4ca+fFsEMAAC26maK8GHzrpupRIkS8vLLL8vmzZtl06ZNctddd0nr1q3l119/Nff369dPFi1aJJ9++qmsXr1ajhw5Im3btvX6aVEzAwAAAqJly5Zut0ePHm2yNRs2bDCBzrvvvitz5841QY6aNWuW3HTTTeb+OnXqeHwdMjMAANhFhOX7JiJxcXFu28WLF6976StXrsi8efMkPj7edDdptiYxMVGaNGniOqZy5cpSsmRJWb9+vXdPKx0vBQAAsHHNTExMjERHR7u2sWPHpnrJ7du3m3qYyMhI6dGjhyxYsECqVKkix44dk2zZsknevHndji9SpIi5zxt0MwEAAK8cPHjQFPQ6aaCSmkqVKsm2bdvk7NmzMn/+fOncubOpj/EnghkAAOzC8s88M87RSZ7Q7Ev58uXN97Vq1ZKNGzfK66+/Lo888ohcunRJzpw545ad0dFMRYsW9apZdDMBAGAXVsYOzU7J1atXTY2NBjZZs2aVFStWuO7btWuXHDhwwNTUeIPMDAAACIjBgwdLixYtTFHvuXPnzMilVatWybJly0ytTbdu3aR///6SP39+k+np3bu3CWS8GcmkCGYAALALK2OXMzhx4oR06tRJjh49aoIXnUBPA5mmTZua+ydNmiQRERFmsjzN1jRv3lymTZvmdbMIZgAAsAsrYxea1Hlk0pI9e3aZOnWq2XxBMAMAgF1YLDQJAAAQdMjMAABgF1bGdjNlFIIZAADswqKbCQAAIOiQmQEAwDYifOwqCs4cCMEMAAB2YdHNBAAAEHTIzAAAYKvMTIRvjw9CBDMAANiFFZ5Ds4OzVQAAAB4iMwMAgF1Y4VkATDADAIBdWOHZzUQwAwCAXVjhmZkJzhALAADAQ2RmAACwC4tuJgAAEMosupkAAACCDpkZAABswrIss/lwAglGBDMAANiEFabBDN1MAAAgpJGZAQDALqz/br48PggRzAAAYBMW3UwAAADBh8wMAAA2YYVpZoZgBgAAm7AIZgAAQCizwjSYoWYGAACENDIzAADYhcXQbAAAEMIsupkAAACCD5kZAABswrL+yc6k/wQSlAhmAACwCUv/86mrKDijGbqZAABASCMzAwCATVhhWgBMMAMAgF1Y4Tk0m24mAAAQ0sjMAABgF5Zv3UwOupkAAEAo18xYBDMAACAzWWEazFAzAwAAQhqZGQAA7MIKz9FMBDMAANiERTcTAABA8CEzAwCATVhhmpkhmAEAwCasMA1m6GYCAAAhjWAGAACbZWYsHzZvjB07Vm677TbJkyePFC5cWNq0aSO7du1yO6ZRo0bXXKNHjx5eXYdgBgAAuw3NtnzYvLB69Wrp2bOnbNiwQZYvXy6JiYnSrFkziY+PdzvuiSeekKNHj7q2cePGeXUdamYAAEBALF261O327NmzTYZm8+bN0qBBA9f+nDlzStGiRdN9HTIzAADYhOWnbqa4uDi37eLFix5d/+zZs+Zr/vz53fbPmTNHChYsKFWrVpXBgwfLhQsXvHpeZGYAALAJy0+jmWJiYtz2v/TSSzJs2LA0H3v16lV55plnpH79+iZocerQoYOUKlVKihcvLj///LM899xzpq7m888/97hdBDMAANiE5adg5uDBgxIVFeXaHxkZed3Hau3ML7/8It9//73b/ieffNL1fbVq1aRYsWJy9913y969e6VcuXIetYtgBgAAeEUDmaTBzPX06tVLFi9eLGvWrJESJUqkeWzt2rXN1z179hDMAACAzF1o0uFwSO/evWXBggWyatUqKVOmzHUfs23bNvNVMzSeIpgBAMAmrAyeAVi7lubOnStffPGFmWvm2LFjZn90dLTkyJHDdCXp/ffee68UKFDA1Mz069fPjHSqXr26x9chmAEAAAExffp018R4Sc2aNUsef/xxyZYtm3zzzTfy2muvmblntLC4Xbt28uKLL3p1HYIZP9IUWuPGjeWvv/6SvHnzZnZz4IOu7e6Qru3ulJhi/wwf3PnHMRn/7hL5Zt1vkjcqpwx+8j5pXKeylCiST06dOS9frfpZxsxYLHHxCZnddMAnb3+yWqZ8uEJOnIqTqhVulFeefUhq3Vw6s5uFEM3MOByONO/X4EUn1vNVps4zo1GZvjAvv/yy2/6FCxd6/YKVLl3aRHae2Lp1qzz00ENSpEgRyZ49u1SoUMHMPvj777+LL+rVq2dmLtT0GULbkRNnZPgbX0jjTuPkrs7j5btNv8ucCU9K5bJFpVihaClaKFqGvr5A6j06Rp4e/qHcXbeKTB7SMbObDfjk8683y4uvLZDnureQVR88Z4KZdr2nysnT5zK7afATS3ycZ8angpvAyfRJ8zSYeOWVV0w2IyNoNXWdOnXMBD86Sc+OHTvkww8/NAHIkCFDUo0sL1++fN1za7pMZzAM1lVF4bml3/0iy9f9Jn8cPCl7D5yQUdMXSfyFi3Jr1TKyY+9R6fzcO+aYfYf/NIGO3n/PnVUlS5ZM/ycFpNu0uSulU5t60rFVXalctphMHPyo5MyeTT78cn1mNw1IU6b/5m3SpIkJAHQxqrR89tlncvPNN5ux7JqFefXVV133aV/c/v37TdFQWik0nVGwS5cuptDoyy+/NNfWymodBjZhwgR58803Xd1Feo4lS5ZIrVq1zDU14ImIiJBNmza5nVOzQTrZj04G5HzcmTNnXNM2a3fTsmXL5KabbpLcuXPLPffcY7I3Thok9enTxxynxU86WVDnzp3NYlwIDhERlrRtWkty5sgmG7fHpnhMVO7sci4+Qa5cuZrh7QP84VLiZdm286A0ur2Sa5/+zmt4e6VU3/cIPVYGLzRpm2AmS5YsMmbMGJkyZYocOnQoxWN0DYeHH35YHn30Udm+fbuZZVCzKBosKJ0lUMetjxgxwrVIVUo0qPjzzz9l0KBBKd6fvM7l+eefN11gmr1p1aqVCX60aCmlIib9R59aAKWB0gcffGDG1x84cEAGDhzoul+zUpoh0vOsXbvWTAut3WzIfFXKFZeDq1+V42tfk4mDH5H/e/Zt2RX7TyV+Uvmjc8mz3VrIewvWZUo7AX/Q2i8Nxgvlz+O2v1D+KFM/gzBhZexCk7YJZtQDDzwgt9xyi5kOOSUTJ040swFqAFOxYkUTPOgEPOPHj3et8aBBkQ770ixPaotV7d6923ytXLmyR+3S4Khp06Zm0h69Rvfu3eWjjz5yrUGxZcsWE1xptic1ukLojBkz5NZbb5WaNWuadq9YscJ1vwZxug6FvgbarjfeeCPN4mG9dvI1MRAYu/cflwYdx0qTLhNk5mffy7Rh/yeVyri/t/Lkyi4fv/aU7Io9Ki+/9VWmtRUA7CwoghlnhuK9994zWZDkdJ+u5ZCU3tbg5MqVK36rqk5OA5CktOtHgyad/EdpZkhHL2m3V2p0JdCkMxjqJEAnTpxwLbh1/Phxuf3221336/m1ays12h2n9T3OLfn6GPCfxMtXJPbQn/LTzoMyYuqX8svuw9Lj0f8NL8ydM1LmT35azl9IkMeefVsu08WEEFYgb25T85W82Pfk6TgpXMDzmV4R3Cy6mQJLJ8hp3ry5yVIEimZ11M6dOz06PleuXNcU+Hbq1Ml0CV26dMlM9NO1a9c0z5E1a1a32/pG8DaoSkpfHw2CnJuuj4GMEWFZki3bDa6MzGdTesmlxCvSof+bcvHS9QvEgWCWLesNckvlGFm9cZdrn9YCrtn4u9xW7fqztiI0WAQzgaf1KYsWLZL1690r57V4VutJktLbGpxoJsMZaFwvS9OsWTOzxPi4ceNSvN9ZuJsW7WrSCX6mTZtminfbtm0r6aWZFR0evnHjRtc+fQ7afZUaLUZ2ronh7doY8NzQnq2k3r/KmXlmtHZGb99Rq4J8umTTfwOZnpIrRzbpPXKO5MmdXQoXyGM2LRYGQtXTHe6S9xeuk48WbzD1Yf1f/lji/74oHVvWyeymwU8sy/ctGAXVpHm6WmbHjh1l8uTJbvsHDBggt912m4wcOVIeeeQRE+xobYkGFE7a1aMFtlokrB/4GrSklGl55513zBwzWtCro4jKly9vioI/+eQTU5w7b968NNuogZUO7dZRR5qV0emYfaFrVmjXkbZDa2a0hkaHqQdr9GsXBfPllunDOkmRglESdz5Bft1zWNr1niarftwp9WtWcP2lunWh+5L31VsNlYNHT2dSqwHftG1WS/48c17GvPmVnDh1TqpVvFHmT+5JNxOCXlAFM86i248//thtnxbOarAxdOhQE9Bo3Ykep4XASR/373//29SnaJFsal05rVu3lnXr1pkAokOHDqaAVutO7rrrLhk1apRHbezWrZs5x/W6mDyhQZGuVaHdV5pl0qXQtbvNmXFC5ugzam6q963dslvy3dYrQ9sDZJQnH25oNoQny2RXfJkBWIKS5fClgMOmNKD69NNPzYJY/qZ91Jr90aHoep3r0WBMu6siqz0hVpZsfm8PEAz+2vhGZjcBCBj9PV6kQLSpgwxU6UDcfz8ryvaZL1ki3etBvXHlYrz8MfnBgLY1LDIzwez8+fOyb98+08XlaRbnenSyv6+//loaNmxoMkp67tjYWJM1AgAAIVYAHOx0jhgdNq0zDvuji0npZHs6xFtrgnS4uc5bowXGmp0BAMCfrDAdzURmxgsadDhnHfYXrddJPlILAIBAsHwckRSksQyZGQAAENrIzAAAYBMREZZP82E5gnQuLYIZAABswqKbCQAAIPiQmQEAwCYsH0ckMZoJAABkKitMu5kIZgAAsAkrTDMz1MwAAICQRmYGAACbsMI0M0MwAwCATVhhWjNDNxMAAAhpZGYAALAJS3zsZpLgTM0QzAAAYBMW3UwAAADBh8wMAAA2YTGaCQAAhDKLbiYAAIDgQ2YGAACbsOhmAgAAocwK024mghkAAGzCCtPMDDUzAAAgpJGZAQDALiwfu4qCMzFDMAMAgF1YdDMBAAAEHzIzAADYhMVoJgAAEMroZgIAAAhCZGYAALAJi24mAAAQyiy6mQAAAIIPmRkAAGzCCtPMDMEMAAA2YYVpzQzdTAAA2CwzY/mweWPs2LFy2223SZ48eaRw4cLSpk0b2bVrl9sxCQkJ0rNnTylQoIDkzp1b2rVrJ8ePH/fqOgQzAAAgIFavXm0ClQ0bNsjy5cslMTFRmjVrJvHx8a5j+vXrJ4sWLZJPP/3UHH/kyBFp27atV9ehmwkAAJuwMribaenSpW63Z8+ebTI0mzdvlgYNGsjZs2fl3Xfflblz58pdd91ljpk1a5bcdNNNJgCqU6eOR9chMwMAgE1YGdzNlJwGLyp//vzmqwY1mq1p0qSJ65jKlStLyZIlZf369R6fl8wMAADwSlxcnNvtyMhIs6Xl6tWr8swzz0j9+vWlatWqZt+xY8ckW7ZskjdvXrdjixQpYu7zFJkZAABswkrS1ZSu7b/niYmJkejoaNemhb7Xo7Uzv/zyi8ybN8/vz4vMDAAANhFhWWbz5fHq4MGDEhUV5dp/vaxMr169ZPHixbJmzRopUaKEa3/RokXl0qVLcubMGbfsjI5m0vs8bpeXzwMAANhcVFSU25ZaMONwOEwgs2DBAlm5cqWUKVPG7f5atWpJ1qxZZcWKFa59OnT7wIEDUrduXY/bQ2YGAACbsDJ4NJN2LelIpS+++MLMNeOsg9GuqRw5cpiv3bp1k/79+5uiYA2MevfubQIZT0cyKYIZAABswsrg5QymT59uvjZq1Mhtvw6/fvzxx833kyZNkoiICDNZ3sWLF6V58+Yybdo0r65DMAMAgE1EWP9svjzeG9rNdD3Zs2eXqVOnmi3d7Ur3IwEAAIIAmRkAAOzC8nHl6yBdaJJgBgAAm7BYNRsAACD4kJkBAMAmrP/+58vjgxHBDAAANhGRwaOZMgrdTAAAIKSRmQEAwCasDJ40L6iCmS+//NLjE7Zq1cqX9gAAgACxwnQ0k0fBTJs2bTyO2K5cueJrmwAAAPwbzFy9etXzMwIAgKAUYVlm8+XxYVczk5CQYNZUAAAAwc8K024mr0czaTfSyJEj5cYbb5TcuXPLH3/8YfYPGTJE3n333UC0EQAA+LEA2PJhC4tgZvTo0TJ79mwZN26cZMuWzbW/atWq8s477/i7fQAAAP4NZt5//3156623pGPHjpIlSxbX/ho1asjOnTu9PR0AAMjgbibLhy0samYOHz4s5cuXT7FIODEx0V/tAgAAfhYRpgXAXmdmqlSpIt999901++fPny//+te//NUuAACAwGRmhg4dKp07dzYZGs3GfP7557Jr1y7T/bR48WJvTwcAADKI9d/Nl8eHRWamdevWsmjRIvnmm28kV65cJrjZsWOH2de0adPAtBIAAPjMCtPRTOmaZ+bOO++U5cuX+781AAAAGTVp3qZNm0xGxllHU6tWrfSeCgAAZIAI65/Nl8eHRTBz6NAhad++vaxdu1by5s1r9p05c0bq1asn8+bNkxIlSgSinQAAwEdWmK6a7XXNTPfu3c0QbM3KnD592mz6vRYD630AAABBnZlZvXq1rFu3TipVquTap99PmTLF1NIAAIDgZQVnciVjg5mYmJgUJ8fTNZuKFy/ur3YBAAA/s+hm+sf48eOld+/epgDYSb/v27evTJgwwd/tAwAAfi4AjvBhC9nMTL58+dyisfj4eKldu7bccMM/D798+bL5vmvXrtKmTZvAtRYAACA9wcxrr73myWEAACCIWWHazeRRMKPLFwAAgNBmhelyBumeNE8lJCTIpUuX3PZFRUX52iYAAIDABTNaL/Pcc8/JJ598IqdOnUpxVBMAAAg+EZZlNl8eHxajmQYNGiQrV66U6dOnS2RkpLzzzjsyfPhwMyxbV84GAADBybJ838IiM6OrY2vQ0qhRI+nSpYuZKK98+fJSqlQpmTNnjnTs2DEwLQUAAPBHZkaXLyhbtqyrPkZvqzvuuEPWrFnj7ekAAEAGj2ayfNjCIpjRQCY2NtZ8X7lyZVM748zYOBeeBAAAwccK024mr4MZ7Vr66aefzPfPP/+8TJ06VbJnzy79+vWTZ599NhBtBAAA8F/NjAYtTk2aNJGdO3fK5s2bTd1M9erVvT0dAADIIBFhOprJp3lmlBb+6gYAAIKb5WNXUZDGMp4FM5MnT/b4hH369PGlPQAAIEAsOy9nMGnSJI+fJMEMAAAIumDGOXoJwWv38ldYSgJha+H2w5ndBCBgLpw/l6GjfiJ8fHxY1swAAIDQYIVpN1OwBlkAAAAeITMDAIBNWJYOr/bt8cGIYAYAAJuI8DGY8eWxgUQ3EwAACGnpCma+++47eeyxx6Ru3bpy+PA/oww++OAD+f777/3dPgAA4CcWC03+47PPPpPmzZtLjhw5ZOvWrXLx4kWz/+zZszJmzJhAtBEAAPixmynCh80ba9askZYtW0rx4sVNILRw4UK3+x9//PFrgqV77rnH++fl7QNGjRolM2bMkLfffluyZs3q2l+/fn3ZsmWL1w0AAADhKT4+XmrUqGEWpU6NBi9Hjx51bR999FHgC4B37dolDRo0uGZ/dHS0nDlzxusGAACA8FybqUWLFmZLS2RkpBQtWjT9jUpPZkYvuGfPnmv2a71M2bJlfWoMAAAI/KrZET5sKi4uzm1zlpykx6pVq6Rw4cJSqVIleeqpp+TUqVPePy9vH/DEE09I37595YcffjB9W0eOHJE5c+bIwIEDTSMAAEBwivDDpmJiYkyPjHMbO3ZsutqjXUzvv/++rFixQl555RVZvXq1yeRcuXIlsN1Mzz//vFy9elXuvvtuuXDhguly0hSRBjO9e/f29nQAACDEHDx40G09QI0D0uPRRx91fV+tWjWpXr26lCtXzmRrNM4IWDCj2ZgXXnhBnn32WdPddP78ealSpYrkzp3b21MBAIAQrJmJiooKyOLGWq5SsGBBE18ENJhxypYtmwliAABAaIiQ/9W9pPfxgXTo0CFTM1OsWDGvHud1MNO4ceM0J81ZuXKlt6cEAABh6Pz5826DhmJjY2Xbtm2SP39+sw0fPlzatWtnBhft3btXBg0aJOXLlzfz2QU0mLnlllvcbicmJpqG/fLLL9K5c2dvTwcAAMJ0aPamTZtMEsSpf//+5qvGC9OnT5eff/5Z3nvvPTO1i06s16xZMxk5cqTXNTheBzOTJk1Kcf+wYcNMBAYAAIJTRAYvNNmoUSNxOByp3r9s2TIJqoUmda2mmTNn+ut0AAAAgS0ATm79+vWSPXt2f50OAAD4mWUyM+lPzQTpOpPeBzNt27Z1u63pI11LQfvFhgwZ4s+2AQCAEK6ZCdpgRmf6SyoiIsJMQTxixAhTuAMAABC0wYxOL9ylSxczS1++fPkC1yoAABDyBcAZxasC4CxZspjsC6tjAwAQeiw//BeMvB7NVLVqVfnjjz8C0xoAABDwzEyED1tYBDOjRo0yi0ouXrzYFP4mXwYcAAAgKGtmtMB3wIABcu+995rbrVq1clvWQEc16W1vl+0GAAAZIyJMa2Y8DmZ0/YQePXrIt99+G9gWAQCAgLAsK831FT15fEgHM87piBs2bBjI9gAAAARuaHawRmQAAOD6bN/NpCpWrHjdgOb06dO+tgkAAASAxQzA/9TNJJ8BGAAAIGSCmUcffVQKFy4cuNYAAICAibAsnxaa9OWxQRHMUC8DAEBoiwjTmpkIb0czAQAAhGRm5urVq4FtCQAACCzLxyJeKwxqZgAAQOiKEMtsvjw+GBHMAABgE1aYDs32eqFJAACAYEJmBgAAm4gI09FMBDMAANhERJjOM0M3EwAACGlkZgAAsAkrTAuACWYAALDT0Gwr/IZm080EAABCGpkZAABswqKbCQAAhLIIH7tkgrU7J1jbBQAA4BEyMwAA2IRlWWbz5fHBiGAGAACbsHxc+Do4QxmCGQAAbCOCGYABAACCD5kZAABsxJLwQzADAIBNWGE6zwzdTAAAIKSRmQEAwCYshmYDAIBQFsEMwAAAAMGHzAwAADZh0c0EAABCmRWmMwDTzQQAAEIamRkAAGzCopsJAACEsogwHc1EMAMAgE1YYZqZCdYgCwAAwCMEMwAA2Gw0k+XD5o01a9ZIy5YtpXjx4iars3DhQrf7HQ6HDB06VIoVKyY5cuSQJk2ayO7du71+XgQzAADYbKFJy4fNG/Hx8VKjRg2ZOnVqivePGzdOJk+eLDNmzJAffvhBcuXKJc2bN5eEhASvrkPNDAAACIgWLVqYLSWalXnttdfkxRdflNatW5t977//vhQpUsRkcB599FGPr0NmBgAAm4gQy+dNxcXFuW0XL170ui2xsbFy7Ngx07XkFB0dLbVr15b169d7+bwAAIAtWH7qZoqJiTGBh3MbO3as123RQEZpJiYpve28z1N0MwEAAK8cPHhQoqKiXLcjIyMlM5GZAQDAJiw//Kc0kEm6pSeYKVq0qPl6/Phxt/1623mfpwhmAACwCSuDRzOlpUyZMiZoWbFihWuf1t/oqKa6det6dS66mQAAQECcP39e9uzZ41b0u23bNsmfP7+ULFlSnnnmGRk1apRUqFDBBDdDhgwxc9K0adPGq+sQzAAAYBNWkhFJ6X28NzZt2iSNGzd23e7fv7/52rlzZ5k9e7YMGjTIzEXz5JNPypkzZ+SOO+6QpUuXSvbs2b26DsEMAAA2YfnYVeTtYxs1amTmk0n9fJaMGDHCbL4gmAEAwCasDA5mMgoFwAAAIKSRmQEAwCasJMOr0/v4YEQwAwCATURY/2y+PD4Y0c0EAABCGpkZAABswqKbCQAAhDKL0UwAAADBh8wMAAA2YfnYVRSkiRmCGQAA7CKC0UwAAADBh8yMH+kaEwsWLPB6tU+EhvVb98i0uSvl510H5fifcTJrbDdp0bB6ZjcLSJdduw7IkqU/yP59x+TM2fPSu1c7qVmzouv+TZt3yapVW2TfvmMSH58gw4d1lZIli2Rqm+E7K0xHM9kyM3Ps2DHp3bu3lC1bViIjIyUmJkZatmwpK1as8Om8R48elRYtWvitnQguFxIuyc3lb5SxAx7M7KYAPrt4MVFiYgrLY481S/H+SxcvSYUKMfLQQ/9b8RjhM5rJ8mELRrbLzOzbt0/q168vefPmlfHjx0u1atUkMTFRli1bJj179pSdO3de8xi9P2vWrNc9d9GiRQPUagSDu+tWMRsQDqpXL2e21NSrV818/fPPMxnYKmRMAXD6BWksY7/MzNNPP226g3788Udp166dVKxYUW6++Wbp37+/bNiwwRyj90+fPl1atWoluXLlklGjRkn58uVlwoQJbufatm2bOXbPnj2uxy1cuNAVNOntzz//XBo3biw5c+aUGjVqyPr1693O8fbbb5vMkN7/wAMPyMSJE02gBQAAPGOrYOb06dOydOlSk4HRICW5pEHEsGHDTHCxfft26datm3Tt2lVmzZrldrzebtCggQl0UvPCCy/IwIEDTeCjgVP79u3l8uXL5r61a9dKjx49pG/fvub+pk2byujRo9N8DhcvXpS4uDi3DQAAT0SIJRGWD1uQ5mZsFcxoBsXhcEjlypWve2yHDh2kS5cupq6mZMmS8vjjj8uuXbtMRsfZ9TR37lwT5KRFA5n77rvPBDLDhw+X/fv3uzI5U6ZMMTU2eozer1mj69XcjB07VqKjo12bZnUAAPCmm8mXLRjZKpjRQMZTt956q9vt4sWLm6Bk5syZ5vaiRYtMluShhx5K8zzVq/9vtEuxYsXM1xMnTpivGhzdfvvtbscnv53c4MGD5ezZs67t4MGDHj8nAADCka2CmQoVKpg6lpSKfJNLqRuqe/fuMm/ePPn7779NF9Mjjzxial3SkrRwWK+trl69Kumlo6+ioqLcNgAA7JyasdVopvz580vz5s1l6tSp0qdPn2sCljNnzqRZfHvvvfeax2hxsNberFmzxqf2VKpUSTZu3Oi2L/ltBI/4Cxcl9tBJ1+0DR0/JL78fkrxROaVE0fyZ2jbAWwkJl+TEib9ct0/+eUYOHDguuXJllwIFouX8+b/l9Ok4+evMOXP/0WOnzNfo6FwSHZ0709oN31hhOs+MrYIZpYGMDs3W7pwRI0aYbiAtyF2+fLkJUnbs2JHqY7NkyWJqZ7SrR7M8devW9aktOteNFhDrCCad52blypWyZMkSVwYHwWXbzgPSrtcbrtsvTf5n5NrD994uk1/smIktA7y3b99ReWXcXNftefP+mWerfv1q0r3b/bJt2255d+ZXrvtnzPjCfG3d6g5p0+bOTGgxkDrbBTNa0LtlyxYzamjAgAFmortChQpJrVq1TDBzPTqyacyYMaY42FcaVM2YMcMUBr/44osma9SvXz95443/fWAieNSvWUGOrXs9s5sB+EXlyqVk1szBqd5/xx3VzYYwY/k48V2Q/q1tu2DGWYirAUNqQUNahcKHDx82dTCdOnVK83GlS5e+5jzahZV83xNPPGG2pLfTGuoNAEB6WWE6aZ4tg5n00JFLJ0+eNPPP6AimIkX8s0aJTsSn88toLY52Mb333nsybdo0v5wbAAA7sNVoJl989NFHUqpUKVMkPG7cOL+dV+et0WBGl1XQLqfJkyebUVMAAPidxWgmW9PCX9387ZNPPvH7OQEASAmjmQAAQEjzdeXrYB1sSzcTAAAIaWRmAACwCYvRTAAAIKRZ4RnN0M0EAABCGpkZAABswmI0EwAACGUWo5kAAACCD5kZAABswgrP+l+CGQAAbMMKz2iGbiYAABDSyMwAAGATFqOZAABAKLPCdDQTwQwAADZhhWfJDDUzAAAgtJGZAQDALqzwTM0QzAAAYBNWmBYA080EAABCGpkZAABswmI0EwAACGVWeJbM0M0EAABCG8EMAAB2S81YPmxeGDZsmFiW5bZVrlzZ70+LbiYAAGzCyoTRTDfffLN88803rts33OD/0INgBgAABIwGL0WLFg3cBehmAgDAfqOZLB82FRcX57ZdvHgx1Wvu3r1bihcvLmXLlpWOHTvKgQMH/P68CGYAALAJy08lMzExMRIdHe3axo4dm+L1ateuLbNnz5alS5fK9OnTJTY2Vu688045d+6cX58X3UwAANiF5Z+x2QcPHpSoqCjX7sjIyBQPb9Gihev76tWrm+CmVKlS8sknn0i3bt3EXwhmAACAVzSQSRrMeCpv3rxSsWJF2bNnj/gT3UwAANhsNJPlw3++OH/+vOzdu1eKFSsm/kQwAwCAXVg+Fv96GcsMHDhQVq9eLfv27ZN169bJAw88IFmyZJH27dv79WnRzQQAAALi0KFDJnA5deqUFCpUSO644w7ZsGGD+d6fCGYAALAJK4PXZpo3b55kBIIZAADswgrPlSapmQEAACGNzAwAADZhZcLaTBmBYAYAAJuwkixJkN7HByO6mQAAQEgjMwMAgE1Y4Vn/SzADAIBtWOEZzRDMAABgE1aYFgBTMwMAAEIamRkAAOzUy2T59vhgRDADAIBNWOFZMkM3EwAACG1kZgAAsAkrTCfNI5gBAMA2rLDsaKKbCQAAhDQyMwAA2IRFNxMAAAhlVlh2MtHNBAAAQhyZGQAAbMKimwkAAIQyK0zXZiKYAQDALqzwLJqhZgYAAIQ0MjMAANiEFZ6JGYIZAADswgrTAmC6mQAAQEgjMwMAgE1YjGYCAAAhzQrPohm6mQAAQEgjMwMAgE1Y4ZmYIZgBAMAuLEYzAQAABB8yMwAA2Ibl44ik4EzNEMwAAGATFt1MAAAAwYdgBgAAhDS6mQAAsAkrTLuZCGYAALAJK0yXM6CbCQAAhDQyMwAA2IRFNxMAAAhlVpguZ0A3EwAACGlkZgAAsAsrPFMzBDMAANiExWgmAACA4ENmBgAAm7AYzQQAAEKZFZ4lM3QzAQBgu2jG8mFLh6lTp0rp0qUle/bsUrt2bfnxxx/9+rQIZgAAQMB8/PHH0r9/f3nppZdky5YtUqNGDWnevLmcOHHCb9cgmAEAwGajmSwf/vPWxIkT5YknnpAuXbpIlSpVZMaMGZIzZ06ZOXOm354XwQwAADYrALZ82Lxx6dIl2bx5szRp0sS1LyIiwtxev369354XBcAhzuFwmK/nzsVldlOAgLlw/lxmNwEImL/jz7v9Pg+kuLg4vzw++XkiIyPNltyff/4pV65ckSJFirjt19s7d+4UfyGYCXHnzv3zS/7mCqUzuykAAB9/n0dHRwfk3NmyZZOiRYtKhTIxPp8rd+7cEhPjfh6thxk2bJhkFoKZEFe8eHE5ePCg5MmTR6xgnQAgzOhfJPoPWV/3qKiozG4O4Fe8vzOeZmQ0kNHf54GSPXt2iY2NNd0+/mhv8s+blLIyqmDBgpIlSxY5fvy42369rcGVvxDMhDjteyxRokRmN8OW9Bc9v+wRrnh/Z6xAZWSSBzS6ZSTNCNWqVUtWrFghbdq0MfuuXr1qbvfq1ctv1yGYAQAAAaPDsjt37iy33nqr3H777fLaa69JfHy8Gd3kLwQzAAAgYB555BE5efKkDB06VI4dOya33HKLLF269JqiYF8QzABe0r5hLXZLrY8YCGW8vxEI2qXkz26l5CxHRowFAwAACBAmzQMAACGNYAYAAIQ0ghkAABDSCGaATLBq1Soz6dSZM2cyuylAmvR9unDhwsxuBpAmghmEhMcff9z8Un355Zfd9usvWW9nPi5durSZ58ATW7dulYceesgMIdTJpipUqGBWf/3999/FF/Xq1ZOjR49myERZsAcd8tq7d28pW7asGYmks/i2bNnSTE7mC32ftmjRwm/tBAKBYAYhQ4OJV155Rf76668Mud7ixYulTp06cvHiRZkzZ47s2LFDPvzwQxOADBkyJMXH6ODAy5cve7xOCktQwB/27dtnZllduXKljB8/XrZv327m8WjcuLH07NkzxcckJiZ6dG59nzJMG0FPh2YDwa5z586O+++/31G5cmXHs88+69q/YMECnVrA7dj58+c7qlSp4siWLZujVKlSjgkTJrjua9iwoTk+6ZaS+Ph4R8GCBR1t2rRJ8f6//vrLfP3222/NOf7zn/84atas6ciaNatj1qxZDsuyHBs3bnR7zKRJkxwlS5Z0XLlyxfU453n0MdHR0Y6lS5ea55grVy5H8+bNHUeOHHE9PjEx0dG7d29zXP78+R2DBg1ydOrUydG6det0vaYIHy1atHDceOONjvPnz19zn/M9pu+3adOmOVq2bOnImTOnY+jQoY5y5co5xo8f73b81q1bzbG7d+92PU7/nanY2Fhz+7PPPnM0atTIkSNHDkf16tUd69atczvHW2+95ShRooS5X/8Nvfrqq+Z9CwQKwQxCJpjRD+3PP//ckT17dsfBgwdTDGY2bdrkiIiIcIwYMcKxa9cuEyToL1T9qk6dOmV+yer9R48eNVtK9Dp63uS/pJNzBiX6C/3rr7927Nmzx1yjadOmjqefftrtWD1GP0CSPi5pMKOBUJMmTUwQtHnzZsdNN93k6NChg+vxo0aNMkGMtm3Hjh2OHj16OKKioghmbE7fbxo8jxkzJs3j9P1WuHBhx8yZMx179+517N+/3zF69GgT+CfVp08fR4MGDdwelzyY0YB78eLF5t/Ygw8+aP5o0GBbff/99+bfoAZJev/UqVPN+5ZgBoFEMIOQCmZUnTp1HF27dk0xmNEPfw0kktJMTtJf2PqLV7MkaXnllVfMeU+fPp3mcc6gZOHChW77P/74Y0e+fPkcCQkJ5rYGJ/qBox8GqQUzeluDISf9EChSpIjrtn6f9K/oy5cvm0wPwYy9/fDDD+a9o0FuWvSYZ555xm3f4cOHHVmyZDHnUJcuXTIZydmzZ6cZzLzzzjuu+3/99VezTwNs9cgjjzjuu+8+t+t07NiRYAYBRc0MQo7Wzbz33numhiU53Ve/fn23fXp79+7dcuXKFY+v4e3E2LqAWlK6Oqwue79gwQJze/bs2aZ+QYuPU5MzZ04pV66c63axYsXkxIkT5vuzZ8/K8ePHzSJtTnp+rZOAvXnzXk3+Pi1evLjcd999MnPmTHN70aJFpkZMi97TUr16dbf3qXK+V3ft2uX2PlXJbwP+RjCDkNOgQQNp3ry5DB48OGDXqFixovm6c+dOj47PlSvXNQW+nTp1klmzZsmlS5dk7ty50rVr1zTPkTVrVrfbWhzMaiO4Hh1hp+8VT96ryd+nqnv37jJv3jz5+++/zftVFwXUwNrT96qziP3q1avpaj/gDwQzCEk6RFv/ily/fr3b/ptuuknWrl3rtk9va3CimQxnoHG9LE2zZs2kYMGCMm7cuBTv92R+GP2Q+Oabb2TatGlmhFPbtm0lvXQElQ4P37hxo2ufPoctW7ak+5wID/nz5zfB/dSpUyU+Pt7r9+q9995rgpzp06ebEVDXC7qvp1KlSm7vU5X8NuBvBDMISdWqVZOOHTvK5MmT3fYPGDDAzKsxcuRIMxeMdke98cYbMnDgQNcx2tWzZs0aOXz4sPz5558pnl9/ub/zzjvy1VdfSatWrUxQosNfN23aJIMGDZIePXpct40aWOnQ7ueee07at28vOXLk8Ok56xwiY8eOlS+++MKk8vv27WuGqTO8GxrIaHCr3TmfffaZ6VbVLlf991G3bt00H6tBvs7jpJlOzfJc73hP3qf/+c9/ZOLEiaYdb775pixZsoT3KQKKYAYha8SIEdektmvWrCmffPKJSZtXrVpVhg4dao7TX9ZJH6eBidanFCpUKNXzt27dWtatW2dS6h06dJDKlSuboETrV0aNGuVRG7t162a6mXz9a1c5gyLtvtIPnNy5c5u/yHX+HdibTpSnWTqty9KAXt/7TZs2NYG9Zlw8fZ926dLF57ZojdqMGTNMMFOjRg2T7enXrx/vUwSUpVXAgb0EYF+aIfr000/l559/9vu5NZDT7M/DDz9srgOk13fffSd33323HDx40HRn+pvOmq01PXodIBBuCMhZAZs7f/68yf5oF5enWZzr2b9/v3z99dfSsGFDM+JEzx0bG2uyRkB66Pvo5MmTMmzYMNeyHf4wYcIEkxnS7lrtYtLuXq0dAwKFbiYgAHr16mWGTTdq1MgvXUwqIiLCDPG+7bbbTCpfp6zXWh7NzgDp8dFHH0mpUqVMkXBqxe7p8eOPP5pgRmvbtMtJa3e0IB4IFLqZAABASCMzAwAAQhrBDAAACGkEMwAAIKQRzAAAgJBGMAPAZzopoS6u6aSjuJ555pkMb8eqVavMTLNpTeGv9y9cuNDjc+qw5VtuucWndukwfb3utm3bfDoPgJQRzABhHGDoB6huuh5V+fLlzezHuk5UoH3++eceT+TnSQACAGlh0jwgjN1zzz1mJWSdHE3Xy+nZs6dZniGlFcd1OnsNevy1+CEAZBQyM0AYi4yMlKJFi5qJ0Z566ilp0qSJfPnll25dQ6NHj5bixYub1Y6VTmmvSyTkzZvXBCW6RpV2kzjpgob9+/c39xcoUMAsvJl8uqrk3UwaTOnaUjExMaZNmiV69913zXl1PSGVL18+k6FxrqOlyzXowpplypQxi3TqOj/z5893u44GaLoiut6v50naTk9pu/QcOXPmNGscDRkyRBITE685ThdM1Pbrcfr66BpdSenCpDqBoa5BpOt4MeMtkHEIZgAb0Q99zcA46UKEugL38uXLZfHixeZDXBevzJMnj1lHZ+3atWZBS83wOB/36quvmpmIZ86cKd9//72cPn1aFixYkOZ1dXFMnW1WZ4LV1Zw1MNDzanCgqzwrbcfRo0fl9ddfN7c1kHn//ffNDLK//vqrWazwsccek9WrV7uCrrZt20rLli1NLYrOMPv88897/Zroc9Xn89tvv5lrv/322zJp0iS3Y/bs2WMWMF20aJFZOHHr1q3y9NNPu+6fM2eOWdRUA0N9fmPGjDFBkU7jDyAD6AzAAMJP586dHa1btzbfX7161bF8+XJHZGSkY+DAga77ixQp4rh48aLrMR988IGjUqVK5ngnvT9HjhyOZcuWmdvFihVzjBs3znV/YmKio0SJEq5rqYYNGzr69u1rvt+1a5embcz1U/Ltt9+a+//66y/XvoSEBEfOnDkd69atczu2W7dujvbt25vvBw8e7KhSpYrb/c8999w150pO71+wYEGq948fP95Rq1Yt1+2XXnrJkSVLFsehQ4dc+5YsWeKIiIhwHD161NwuV66cY+7cuW7nGTlypKNu3brm+9jYWHPdrVu3pnpdAOlHzQwQxjTbohkQzbhot40uSqmjc5x07ZykdTI//fSTyUJotiKphIQE2bt3r+la0exJ7dq1XffdcMMNcuutt17T1eSkWZMsWbKYBTI9pW24cOGCWd8nKc0O/etf/zLfawYkaTtU3bp1xVsff/yxyRjp89MFQrVAOioqyu2YkiVLyo033uh2HX09NZukr5U+tlu3bmZ1aCc9T3R0tNftAeA9ghkgjGkdyfTp003AonUxGngkpasaJ6Uf5rpApnabJFeoUKF0d215S9uhvvrqK7cgQmnNjb+sX79eOnbsKMOHDzfdaxp8zJs3z3SledtW7Z5KHlxpEAcg8AhmgDCmwYoW23qqZs2aJlNRuHDha7ITTsWKFZMffvhBGjRo4MpAbN682Tw2JZr90SyG1rpoAXJyzsyQFhY7ValSxQQtBw4cSDWjo8W2zmJmpw0bNog31q1bZ4qjX3jhBde+/fv3X3OctuPIkSMmIHReR1cx16LpIkWKmP1//PGHCYwAZDwKgAG46IdxwYIFzQgmLQCOjY0188D06dNHDh06ZI7p27evvPzyy2biuZ07d5pC2LTmiCldurR07txZunbtah7jPKcW1CoNJnQUk3aJnTx50mQ6tOtm4MCBpuhXi2i1G2fLli0yZcoUV1Ftjx49ZPfu3fLss8+a7p65c+eaQl5vVKhQwQQqmo3Ra2h3U0rFzDpCSZ+DdsPp66Kvh45o0pFiSjM7WrCsj//9999l+/btZkj8xIkTvWoPgPQhmAHgosOO16xZY2pEdKSQZj+0FkRrZpyZmgEDBsj//d//mQ93rR3RwOOBBx5I87za1fXggw+awEeHLWttSXx8vLlPu5E0GNCRSJrl6NWrl9mvk+7piCANErQdOqJKu510qLbSNupIKA2QdNi2jnrSUUTeaNWqlQmY9Jo6y69mavSayWl2S1+Pe++9V5o1aybVq1d3G3qtI6l0aLYGMJqJ0mySBlbOtgIILEurgAN8DQAAgIAhMwMAAEIawQwAAAhpBDMAACCkEcwAAICQRjADAABCGsEMAAAIaQQzAAAgpBHMAACAkEYwAwAAQhrBDAAACGkEMwAAIKQRzAAAAAll/x+Zp5cwpjRpjQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "test_preds, test_targets = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X, y in test_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        p = model(X)\n",
    "        predicted_labels = (p > 0.5).int().cpu().numpy().tolist()\n",
    "        test_preds.extend(predicted_labels)\n",
    "        test_targets.extend(y.int().cpu().numpy().tolist())\n",
    "\n",
    "print(f\"Accuracy:  {accuracy_score(test_targets, test_preds):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(test_targets, test_preds):.4f}\")\n",
    "print(f\"Precision: {precision_score(test_targets, test_preds):.4f}\")\n",
    "print(f\"Recall:    {recall_score(test_targets, test_preds):.4f}\")\n",
    "\n",
    "cm = confusion_matrix(test_targets, test_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Not Crying\", \"Crying\"])\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "disp.plot(cmap=\"Blues\", values_format='d')\n",
    "plt.title(\"Confusion Matrix - Test Set\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f3d2996e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 600x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_targets, test_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Not Crying\", \"Crying\"])\n",
    "\n",
    "# Plot it\n",
    "plt.figure(figsize=(6, 6))\n",
    "disp.plot(cmap=\"Blues\", values_format='d')\n",
    "plt.title(\"Confusion Matrix - Test Set\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01a1bf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ramil\\AppData\\Local\\Temp\\ipykernel_31664\\297496611.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"cry_model.pth\", map_location=\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TorchScript model saved as cry_model_torchscript.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from model import CryClassifier  # Make sure this matches your actual model code\n",
    "\n",
    "# Load the trained model\n",
    "model = CryClassifier()\n",
    "model.load_state_dict(torch.load(\"cry_model.pth\", map_location=\"cpu\"))\n",
    "model.eval()\n",
    "\n",
    "# Create a dummy input tensor (same shape as your model input)\n",
    "example_input = torch.randn(1, 3, 128, 128)  # adjust shape if needed\n",
    "\n",
    "# Convert to TorchScript\n",
    "traced_model = torch.jit.trace(model, example_input)\n",
    "\n",
    "# Save the TorchScript model\n",
    "traced_model.save(\"cry_model_torchscript.pt\")\n",
    "print(\"✅ TorchScript model saved as cry_model_torchscript.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923f8626",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [16, 3, 3, 3], expected input[1, 1, 128, 128] to have 3 channels, but got 1 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      5\u001b[0m example_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# adjust shape as per your model\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m traced_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m traced_model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcry_model_rpi.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ramil\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\jit\\_trace.py:1002\u001b[0m, in \u001b[0;36mtrace\u001b[1;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_kwarg_inputs, _store_inputs)\u001b[0m\n\u001b[0;32m    989\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    990\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`optimize` is deprecated and has no effect. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse `with torch.jit.optimized_execution()` instead\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    992\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    993\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    994\u001b[0m     )\n\u001b[0;32m    996\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils_internal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    997\u001b[0m     check_if_torch_exportable,\n\u001b[0;32m    998\u001b[0m     log_torch_jit_trace_exportability,\n\u001b[0;32m    999\u001b[0m     log_torchscript_usage,\n\u001b[0;32m   1000\u001b[0m )\n\u001b[1;32m-> 1002\u001b[0m traced_func \u001b[38;5;241m=\u001b[39m \u001b[43m_trace_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1003\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1004\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1005\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1006\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_trace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1007\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_tolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_module_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_compilation_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample_kwarg_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_store_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1016\u001b[0m log_torchscript_usage(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrace\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_id\u001b[38;5;241m=\u001b[39m_get_model_id(traced_func))\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_if_torch_exportable():\n",
      "File \u001b[1;32mc:\\Users\\ramil\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\jit\\_trace.py:698\u001b[0m, in \u001b[0;36m_trace_impl\u001b[1;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_kwarg_inputs, _store_inputs)\u001b[0m\n\u001b[0;32m    696\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    697\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample_kwarg_inputs should be a dict\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 698\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrace_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforward\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_trace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrap_check_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheck_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_tolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    706\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    707\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_module_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    708\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexample_inputs_is_kwarg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexample_kwarg_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_store_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_store_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(func, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__self__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule)\n\u001b[0;32m    714\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    715\u001b[0m ):\n\u001b[0;32m    716\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m example_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ramil\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\jit\\_trace.py:1278\u001b[0m, in \u001b[0;36mtrace_module\u001b[1;34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_inputs_is_kwarg, _store_inputs)\u001b[0m\n\u001b[0;32m   1276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1277\u001b[0m     example_inputs \u001b[38;5;241m=\u001b[39m make_tuple(example_inputs)\n\u001b[1;32m-> 1278\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_c\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_method_from_trace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvar_lookup_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1284\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1285\u001b[0m \u001b[43m        \u001b[49m\u001b[43margument_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1286\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_store_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1287\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1289\u001b[0m check_trace_method \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_c\u001b[38;5;241m.\u001b[39m_get_method(method_name)\n\u001b[0;32m   1291\u001b[0m \u001b[38;5;66;03m# Check the trace against new traces created from user-specified inputs\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ramil\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ramil\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ramil\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1726\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1724\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1725\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1726\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1727\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1728\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[1;32mc:\\Users\\ramil\\Downloads\\fresh\\model.py:28\u001b[0m, in \u001b[0;36mCryClassifier.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# x: (B, 3, 128, T)\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m                      \u001b[38;5;66;03m# → (B, 32, H, W)\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     B, C, H, W \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     30\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(B, W, C \u001b[38;5;241m*\u001b[39m H)  \u001b[38;5;66;03m# (B, T, Features)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ramil\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ramil\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ramil\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1726\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1724\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1725\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1726\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1727\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1728\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[1;32mc:\\Users\\ramil\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ramil\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ramil\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ramil\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1726\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1724\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1725\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1726\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1727\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1728\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[1;32mc:\\Users\\ramil\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ramil\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [16, 3, 3, 3], expected input[1, 1, 128, 128] to have 3 channels, but got 1 channels instead"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "26878c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import dlib\n",
    "import numpy as np\n",
    "import warnings\n",
    "from collections import deque\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# MediaPipe setup\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1)\n",
    "pose = mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.6, model_complexity=1)\n",
    "\n",
    "# Dlib fallback\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "# Eye landmarks\n",
    "LEFT_EYE = [33, 160, 158, 133, 153, 144]\n",
    "RIGHT_EYE = [362, 385, 387, 263, 373, 380]\n",
    "\n",
    "# Thresholds\n",
    "EAR_THRESH = 0.2\n",
    "SLEEP_FRAME_THRESH = 10\n",
    "CLOSED_FRAMES = 0\n",
    "\n",
    "crying = False  # Connect your audio cry detection to this\n",
    "\n",
    "def send_alert(alert_type, message):\n",
    "    print(f\"ALERT [{alert_type}]: {message}\")\n",
    "\n",
    "def eye_aspect_ratio(landmarks, eye_indices):\n",
    "    p = [landmarks[i] for i in eye_indices]\n",
    "    hor_dist = np.linalg.norm(np.array(p[0]) - np.array(p[3]))\n",
    "    ver_dist = (np.linalg.norm(np.array(p[1]) - np.array(p[5])) + np.linalg.norm(np.array(p[2]) - np.array(p[4]))) / 2\n",
    "    ear = ver_dist / hor_dist\n",
    "    return ear\n",
    "\n",
    "def detect_camera_obstruction(pose_results, faces):\n",
    "    return not pose_results.pose_landmarks and not faces\n",
    "\n",
    "def detect_sleep_state(frame):\n",
    "    global CLOSED_FRAMES\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    mesh_results = face_mesh.process(rgb)\n",
    "\n",
    "    if mesh_results.multi_face_landmarks:\n",
    "        for face_landmarks in mesh_results.multi_face_landmarks:\n",
    "            h, w, _ = frame.shape\n",
    "            mesh_landmarks = [(int(p.x * w), int(p.y * h)) for p in face_landmarks.landmark]\n",
    "\n",
    "            left_ear = eye_aspect_ratio(mesh_landmarks, LEFT_EYE)\n",
    "            right_ear = eye_aspect_ratio(mesh_landmarks, RIGHT_EYE)\n",
    "            avg_ear = (left_ear + right_ear) / 2\n",
    "\n",
    "            if avg_ear < EAR_THRESH:\n",
    "                CLOSED_FRAMES += 1\n",
    "            else:\n",
    "                CLOSED_FRAMES = 0\n",
    "\n",
    "            if CLOSED_FRAMES > SLEEP_FRAME_THRESH:\n",
    "                return \"Sleeping\"\n",
    "            else:\n",
    "                return \"Awake\"\n",
    "    return \"Face Not Detected\"\n",
    "\n",
    "def process_frame(frame):\n",
    "    status = \"Monitoring\"\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    pose_results = pose.process(rgb_frame)\n",
    "    faces = detector(gray_frame)\n",
    "\n",
    "    if detect_camera_obstruction(pose_results, faces):\n",
    "        send_alert(\"CAMERA_OBSTRUCTION\", \"Camera is obstructed!\")\n",
    "        return frame, \"Camera Obstructed\"\n",
    "\n",
    "    if len(faces) == 0:\n",
    "        send_alert(\"SAFETY_CONCERN\", \"Cannot detect infant's face!\")\n",
    "        return frame, \"Face Covered/Down\"\n",
    "\n",
    "    sleep_state = detect_sleep_state(frame)\n",
    "    status = sleep_state\n",
    "\n",
    "    if crying and sleep_state == \"Awake\":\n",
    "        cv2.putText(frame, \"Alert: Baby Awake and Crying\", (10, 130),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "        send_alert(\"CRYING\", \"Baby is awake and crying!\")\n",
    "\n",
    "    if pose_results.pose_landmarks:\n",
    "        landmarks = pose_results.pose_landmarks.landmark\n",
    "        hips = landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].visibility\n",
    "        knees = landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].visibility\n",
    "        feet = landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].visibility\n",
    "        if hips > 0.5 and knees > 0.5 and feet > 0.5:\n",
    "            status = \"Uncovered Body\"\n",
    "            send_alert(\"UNCOVERED_BODY\", \"Infant's body is uncovered!\")\n",
    "\n",
    "        mp_drawing.draw_landmarks(frame, pose_results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "    return frame, status\n",
    "\n",
    "def run_on_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(\"Error: Could not read image.\")\n",
    "        return\n",
    "\n",
    "    # Resize image to fit screen (preserving aspect ratio)\n",
    "    max_dimension = 800\n",
    "    height, width = image.shape[:2]\n",
    "    scale = min(max_dimension / width, max_dimension / height)\n",
    "    new_width = int(width * scale)\n",
    "    new_height = int(height * scale)\n",
    "    image = cv2.resize(image, (new_width, new_height))\n",
    "\n",
    "    frame, status = process_frame(image)\n",
    "    cv2.putText(frame, f\"Status: {status}\", (10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"Infant Monitoring - Image\", frame)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your actual image path\n",
    "    run_on_image(\"sleeping.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f24c53e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import dlib\n",
    "import numpy as np\n",
    "import warnings\n",
    "from collections import deque\n",
    "\n",
    "# Initialize MediaPipe for pose detection\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(\n",
    "    min_detection_confidence=0.6,\n",
    "    min_tracking_confidence=0.6,\n",
    "    model_complexity=1\n",
    ")\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Initialize Dlib detector & facial landmarks predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor_dlib = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# Buffers for movement tracking\n",
    "sleep_state_buffer = deque(maxlen=15)\n",
    "\n",
    "def send_alert(alert_type, message):\n",
    "    print(f\"ALERT [{alert_type}]: {message}\")\n",
    "\n",
    "def eye_aspect_ratio(eye):\n",
    "    from scipy.spatial import distance\n",
    "    A = distance.euclidean(eye[1], eye[5])  \n",
    "    B = distance.euclidean(eye[2], eye[4])  \n",
    "    C = distance.euclidean(eye[0], eye[3])  \n",
    "    return (A + B) / (2.0 * C)\n",
    "\n",
    "def detect_camera_obstruction(frame, pose_results, faces):\n",
    "    # Check for total absence of landmarks and faces\n",
    "    if not pose_results.pose_landmarks and not faces:\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        brightness = np.mean(gray)\n",
    "        if brightness < 40:  # Threshold for likely obstruction (e.g., cloth or covered camera)\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def detect_sleep_state(frame, faces):\n",
    "    if len(faces) == 0:\n",
    "        return \"Unknown\"\n",
    "\n",
    "    for face in faces:\n",
    "        landmarks = predictor_dlib(frame, face)\n",
    "        left_eye = [(landmarks.part(i).x, landmarks.part(i).y) for i in range(36, 42)]\n",
    "        right_eye = [(landmarks.part(i).x, landmarks.part(i).y) for i in range(42, 48)]\n",
    "\n",
    "        left_ear = eye_aspect_ratio(left_eye)\n",
    "        right_ear = eye_aspect_ratio(right_eye)\n",
    "        avg_ear = (left_ear + right_ear) / 2.0\n",
    "\n",
    "        current_state = \"Sleeping\" if avg_ear < 0.22 else \"Awake\"\n",
    "        sleep_state_buffer.append(current_state)\n",
    "\n",
    "        sleeping_count = sum(1 for state in sleep_state_buffer if state == \"Sleeping\")\n",
    "        final_state = \"Sleeping\" if sleeping_count > len(sleep_state_buffer) * 0.8 else \"Awake\"\n",
    "\n",
    "        if final_state == \"Sleeping\" and len(sleep_state_buffer) == sleep_state_buffer.maxlen:\n",
    "            if all(state == \"Sleeping\" for state in list(sleep_state_buffer)[-5:]):\n",
    "                send_alert(\"SLEEPING\", \"Infant appears to be sleeping\")\n",
    "\n",
    "        return final_state\n",
    "\n",
    "    return \"Unknown\"\n",
    "\n",
    "def process_frame(frame):\n",
    "    status = \"Monitoring\"\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    pose_results = pose.process(rgb_frame)\n",
    "    faces = detector(gray_frame)\n",
    "\n",
    "    if detect_camera_obstruction(frame, pose_results, faces):\n",
    "        send_alert(\"CAMERA_OBSTRUCTION\", \"Camera is obstructed!\")\n",
    "        return frame, \"Camera Obstructed\"\n",
    "\n",
    "\n",
    "    if len(faces) == 0:\n",
    "        send_alert(\"SAFETY_CONCERN\", \"Cannot detect infant's face!\")\n",
    "        return frame, \"Face Covered/Down\"\n",
    "\n",
    "    sleep_state = detect_sleep_state(gray_frame, faces)\n",
    "    status = sleep_state\n",
    "\n",
    "    if pose_results.pose_landmarks:\n",
    "        landmarks = pose_results.pose_landmarks.landmark\n",
    "        nose = landmarks[mp_pose.PoseLandmark.NOSE.value]\n",
    "        if nose.visibility < 0.5:\n",
    "            status = \"Face Down\"\n",
    "            send_alert(\"FACE_DOWN\", \"Infant is face down!\")\n",
    "\n",
    "        hips = landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].visibility\n",
    "        knees = landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].visibility\n",
    "        feet = landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].visibility\n",
    "        if hips > 0.5 and knees > 0.5 and feet > 0.5:\n",
    "            status = \"Uncovered Body\"\n",
    "            send_alert(\"UNCOVERED_BODY\", \"Infant's body is uncovered!\")\n",
    "\n",
    "        mp_drawing.draw_landmarks(frame, pose_results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "    return frame, status\n",
    "\n",
    "def run_on_image(image_path):\n",
    "    \"\"\" Process a single image for infant monitoring. \"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(\"Error: Image not found or unreadable.\")\n",
    "        return\n",
    "\n",
    "    frame, status = process_frame(image)\n",
    "\n",
    "    # Resize frame to fit typical laptop screen (max width = 800)\n",
    "    max_width = 800\n",
    "    height, width = frame.shape[:2]\n",
    "    if width > max_width:\n",
    "        scale_ratio = max_width / width\n",
    "        frame = cv2.resize(frame, (int(width * scale_ratio), int(height * scale_ratio)))\n",
    "\n",
    "    cv2.putText(frame, f\"Status: {status}\", (10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"Infant Monitoring - Image\", frame)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e1b8c785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALERT [SAFETY_CONCERN]: Cannot detect infant's face!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Replace this with your image path\n",
    "    run_on_image(\"facecovered.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173d25f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
